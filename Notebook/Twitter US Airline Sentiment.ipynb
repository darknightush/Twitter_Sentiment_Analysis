{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4057e99d",
   "metadata": {},
   "source": [
    "## Twitter Sentiment Analysis (Text classification)\n",
    "\n",
    "https://www.kaggle.com/crowdflower/twitter-airline-sentiment\n",
    "This dataset is taken from Kaggle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ffca37f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imported necessary libraries\n",
    "\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, confusion_matrix\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM, Bidirectional, Dropout, Embedding, SpatialDropout1D, LSTM\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras import models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "62dfc2e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>airline_sentiment</th>\n",
       "      <th>airline_sentiment_confidence</th>\n",
       "      <th>negativereason</th>\n",
       "      <th>negativereason_confidence</th>\n",
       "      <th>airline</th>\n",
       "      <th>airline_sentiment_gold</th>\n",
       "      <th>name</th>\n",
       "      <th>negativereason_gold</th>\n",
       "      <th>retweet_count</th>\n",
       "      <th>text</th>\n",
       "      <th>tweet_coord</th>\n",
       "      <th>tweet_created</th>\n",
       "      <th>tweet_location</th>\n",
       "      <th>user_timezone</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>570306133677760513</td>\n",
       "      <td>neutral</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>cairdin</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica What @dhepburn said.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 11:35:52 -0800</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Eastern Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>570301130888122368</td>\n",
       "      <td>positive</td>\n",
       "      <td>0.3486</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>jnardino</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica plus you've added commercials t...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 11:15:59 -0800</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Pacific Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>570301083672813571</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0.6837</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>yvonnalynn</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica I didn't today... Must mean I n...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 11:15:48 -0800</td>\n",
       "      <td>Lets Play</td>\n",
       "      <td>Central Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>570301031407624196</td>\n",
       "      <td>negative</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>Bad Flight</td>\n",
       "      <td>0.7033</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>jnardino</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica it's really aggressive to blast...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 11:15:36 -0800</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Pacific Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>570300817074462722</td>\n",
       "      <td>negative</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>Can't Tell</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>jnardino</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica and it's a really big bad thing...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 11:14:45 -0800</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Pacific Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             tweet_id airline_sentiment  airline_sentiment_confidence  \\\n",
       "0  570306133677760513           neutral                        1.0000   \n",
       "1  570301130888122368          positive                        0.3486   \n",
       "2  570301083672813571           neutral                        0.6837   \n",
       "3  570301031407624196          negative                        1.0000   \n",
       "4  570300817074462722          negative                        1.0000   \n",
       "\n",
       "  negativereason  negativereason_confidence         airline  \\\n",
       "0            NaN                        NaN  Virgin America   \n",
       "1            NaN                     0.0000  Virgin America   \n",
       "2            NaN                        NaN  Virgin America   \n",
       "3     Bad Flight                     0.7033  Virgin America   \n",
       "4     Can't Tell                     1.0000  Virgin America   \n",
       "\n",
       "  airline_sentiment_gold        name negativereason_gold  retweet_count  \\\n",
       "0                    NaN     cairdin                 NaN              0   \n",
       "1                    NaN    jnardino                 NaN              0   \n",
       "2                    NaN  yvonnalynn                 NaN              0   \n",
       "3                    NaN    jnardino                 NaN              0   \n",
       "4                    NaN    jnardino                 NaN              0   \n",
       "\n",
       "                                                text tweet_coord  \\\n",
       "0                @VirginAmerica What @dhepburn said.         NaN   \n",
       "1  @VirginAmerica plus you've added commercials t...         NaN   \n",
       "2  @VirginAmerica I didn't today... Must mean I n...         NaN   \n",
       "3  @VirginAmerica it's really aggressive to blast...         NaN   \n",
       "4  @VirginAmerica and it's a really big bad thing...         NaN   \n",
       "\n",
       "               tweet_created tweet_location               user_timezone  \n",
       "0  2015-02-24 11:35:52 -0800            NaN  Eastern Time (US & Canada)  \n",
       "1  2015-02-24 11:15:59 -0800            NaN  Pacific Time (US & Canada)  \n",
       "2  2015-02-24 11:15:48 -0800      Lets Play  Central Time (US & Canada)  \n",
       "3  2015-02-24 11:15:36 -0800            NaN  Pacific Time (US & Canada)  \n",
       "4  2015-02-24 11:14:45 -0800            NaN  Pacific Time (US & Canada)  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Loading data\n",
    "\n",
    "df= pd.read_csv('Tweets.csv', sep=',')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a236c317",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Rows: 14640\n",
      "Number of Columns: 15\n"
     ]
    }
   ],
   "source": [
    "# Shape of Dataset\n",
    "\n",
    "print(\"Number of Rows: \" + str(df.shape[0]))\n",
    "print(\"Number of Columns: \" + str(df.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d692d466",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Select Required Columns Only\n",
    "\n",
    "tweet_df = df[['text','airline_sentiment']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e8384f90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>airline_sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>@VirginAmerica What @dhepburn said.</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@VirginAmerica plus you've added commercials t...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@VirginAmerica I didn't today... Must mean I n...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@VirginAmerica it's really aggressive to blast...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@VirginAmerica and it's a really big bad thing...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text airline_sentiment\n",
       "0                @VirginAmerica What @dhepburn said.           neutral\n",
       "1  @VirginAmerica plus you've added commercials t...          positive\n",
       "2  @VirginAmerica I didn't today... Must mean I n...           neutral\n",
       "3  @VirginAmerica it's really aggressive to blast...          negative\n",
       "4  @VirginAmerica and it's a really big bad thing...          negative"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f29cc693",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select only Positive and Negative Reviews\n",
    "\n",
    "tweet_df = tweet_df[tweet_df['airline_sentiment'] != 'neutral']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d440fc95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attribute and Labels\n",
    "X = tweet_df.text\n",
    "y = tweet_df.airline_sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "298e0d73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Test Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4dff5a03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Set X Items: 9232\n",
      "Training Set y Items: 9232\n"
     ]
    }
   ],
   "source": [
    "# Checking Train Set\n",
    "\n",
    "print(\"Training Set X Items: \" + str(len(X_train)))\n",
    "print(\"Training Set y Items: \" + str(len(y_train)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "05b24217",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Set X Items: 2309\n",
      "Test Set y Items: 2309\n"
     ]
    }
   ],
   "source": [
    "# Checking Test Set\n",
    "\n",
    "print(\"Test Set X Items: \" + str(len(X_test)))\n",
    "print(\"Test Set y Items: \" + str(len(y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e24cdd6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting required labels only and encoding\n",
    "\n",
    "review_labels_train = y_train.factorize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e81bf337",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, ..., 1, 0, 0], dtype=int64)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "review_labels_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cb7c532a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['negative', 'positive'], dtype='object')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check Review Labels\n",
    "review_labels_train[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a05781df",
   "metadata": {},
   "source": [
    "## Model Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a04c72ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['@USAirways Another dead end.  They only handle AA L&amp;F.  They gave me the same failed # I already had. 610-362-7498(99) VM full.  #lost',\n",
       "       '@USAirways #2066. Was on plane from PBI to CLT and knew about the frozen water. Also saw a plane to NYC take off at the gate next door!',\n",
       "       '@USAirways waiting for bags now over 25min in Phl bag claim!',\n",
       "       ...,\n",
       "       'Lovely! RT @JetBlue: Our fleet’s on fleek. http://t.co/Hi6Fl1AX9E',\n",
       "       \"@united Okay thanks if you could please update me. I was told at the airport someone would call me today but they haven't.\",\n",
       "       '@USAirways IS THIS RINGLING BROTHERS BARNUM AND BAILEY???  SHOULD I KEEP MY EYES PEELED FOR THE CLOWN CAR???'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size = X_train.values\n",
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7fbac0bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<keras_preprocessing.text.Tokenizer object at 0x0000027409E8F5E0>\n",
      "11636\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Vectorize a text corpus, by turning each text into sequence of integers\n",
    "\n",
    "tokenizer = Tokenizer(num_words=8000,oov_token='OOV')\n",
    "tokenizer.fit_on_texts(vocab_size)\n",
    "\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "\n",
    "print(tokenizer)\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a63b8ffb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11635"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenizer.word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "87d4eca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To save the tokenized vocab for Web app\n",
    "#import pickle\n",
    "#with open('tokenizer.pickle', 'wb') as handle:\n",
    "#        pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "336c0a46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   0    0    0 ... 3560  364  141]\n",
      " [   0    0    0 ...   76  196  754]\n",
      " [   0    0    0 ...  304   79  354]\n",
      " ...\n",
      " [   0    0    0 ...   48   50    1]\n",
      " [   0    0    0 ...   32   57  353]\n",
      " [   0    0    0 ...    3 4197  569]]\n"
     ]
    }
   ],
   "source": [
    "# Store and Padding Converted Sequences\n",
    "tweet = X_train.values\n",
    "\n",
    "tweet_seqs = tokenizer.texts_to_sequences(tweet)\n",
    "\n",
    "padded_sequence_train = pad_sequences(tweet_seqs, maxlen=200)\n",
    "print(padded_sequence_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f473795f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0   13  143 1590  549   57  122  660  221 2105\n",
      "   59  752   57  428   20    3  257  855    4  224   81 5034 3559 5035\n",
      " 2830 3560  364  141]\n"
     ]
    }
   ],
   "source": [
    "# Check padded sequence element\n",
    "\n",
    "print(padded_sequence_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6be8b816",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 200, 32)           372352    \n",
      "_________________________________________________________________\n",
      "spatial_dropout1d (SpatialDr (None, 200, 32)           0         \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (None, 50)                16600     \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1)                 51        \n",
      "=================================================================\n",
      "Total params: 389,003\n",
      "Trainable params: 389,003\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "embedding_vector_length = 32\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, embedding_vector_length,     \n",
    "                                     input_length=200) )\n",
    "model.add(SpatialDropout1D(0.25))\n",
    "model.add(LSTM(50, dropout=0.5, recurrent_dropout=0.5))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy',optimizer='adam', metrics=['accuracy'])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "13e403dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Saved\n"
     ]
    }
   ],
   "source": [
    "# Used to save trained model \n",
    "\n",
    "#model.save(\"sentiment_analysis.h5\")\n",
    "#print(\"Model Saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5253bb25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "231/231 [==============================] - 61s 248ms/step - loss: 0.4095 - accuracy: 0.8271 - val_loss: 0.3046 - val_accuracy: 0.8749\n",
      "Epoch 2/5\n",
      "231/231 [==============================] - 58s 250ms/step - loss: 0.2315 - accuracy: 0.9101 - val_loss: 0.2580 - val_accuracy: 0.8977\n",
      "Epoch 3/5\n",
      "231/231 [==============================] - 59s 256ms/step - loss: 0.1572 - accuracy: 0.9431 - val_loss: 0.2318 - val_accuracy: 0.9053\n",
      "Epoch 4/5\n",
      "231/231 [==============================] - 59s 255ms/step - loss: 0.1197 - accuracy: 0.9582 - val_loss: 0.2826 - val_accuracy: 0.9123\n",
      "Epoch 5/5\n",
      "231/231 [==============================] - 59s 257ms/step - loss: 0.0915 - accuracy: 0.9679 - val_loss: 0.2531 - val_accuracy: 0.9090\n"
     ]
    }
   ],
   "source": [
    "# Training the model\n",
    "\n",
    "trained = model.fit(padded_sequence_train,review_labels_train[0], validation_split=0.2, epochs=5, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "042ff09d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   0    0    0 ...   10    1   18]\n",
      " [   0    0    0 ...   48   50    1]\n",
      " [   0    0    0 ...    2 6450  529]\n",
      " ...\n",
      " [   0    0    0 ...   99    3  314]\n",
      " [   0    0    0 ... 1542   91    1]\n",
      " [   0    0    0 ...  200  106   30]]\n"
     ]
    }
   ],
   "source": [
    "# Encoding and Padding Test Data to Check Accuracy\n",
    "\n",
    "encoded_docs = tokenizer.texts_to_sequences(X_test)\n",
    "padded_sequence_test = pad_sequences(encoded_docs, maxlen=200)\n",
    "print(padded_sequence_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "de9b5585",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 0, ..., 0, 1, 0], dtype=int64)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentiment_label_test = y_test.factorize()\n",
    "sentiment_label_test[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "64d4e514",
   "metadata": {},
   "outputs": [],
   "source": [
    "score = model.evaluate(padded_sequence_test,sentiment_label_test[0],verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "efa3614d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9242095947265625\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy: {}\".format(score[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03239f11",
   "metadata": {},
   "source": [
    "### Loading model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e75e1a42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Loaded\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 200, 32)           372352    \n",
      "_________________________________________________________________\n",
      "spatial_dropout1d_1 (Spatial (None, 200, 32)           0         \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 50)                16600     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 51        \n",
      "=================================================================\n",
      "Total params: 389,003\n",
      "Trainable params: 389,003\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#Code to load the saved model\n",
    "model = models.load_model('sentiment_analysis.h5')\n",
    "print(\"Model Loaded\")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "891f19eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "score = model.evaluate(padded_sequence_test,sentiment_label_test[0],verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3f4bef39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9268081188201904\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy: {}\".format(score[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f3fc9b0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import pickle\n",
    "#with open('tokenizer.pickle', 'rb') as handle:\n",
    " #   tokenizer = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "81bff584",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11635"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenizer.word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "f50f2f9e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,  548,    1,  146,    5, 2755,  466,    8,  539,   57,\n",
       "          39,    1,    1, 1716, 2006,   10, 1469,   22, 2550, 4229,   97,\n",
       "           1,   12,    1,   10,  223,   57,    1,    3,    1,    2,    3,\n",
       "           1, 3877,  495,   10,    1,  106,    3, 2569,  705, 1672,  548,\n",
       "          39,  111]])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test Review Sentence\n",
    "\n",
    "test_word =\"\"\"\n",
    "These masks were a steal! 50 for 12$!! They are soft, breathable, light, comfortable and professional. It doesn’t hurt or irritate my ears and where they glue the straps to the mask isn’t hard and crusty like the basic blue ones. These are great!\n",
    "\"\"\"\n",
    "\n",
    "# To\n",
    "tw = tokenizer.texts_to_sequences([test_word])\n",
    "tw = pad_sequences(tw,maxlen=200)\n",
    "\n",
    "tw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "6796f3a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual Review: \n",
      "These masks were a steal! 50 for 12$!! They are soft, breathable, light, comfortable and professional. It doesn’t hurt or irritate my ears and where they glue the straps to the mask isn’t hard and crusty like the basic blue ones. These are great!\n",
      "\n",
      "\n",
      "Sentiment Analysis Outcome ==> The review shows Positive sentiment.\n",
      "\n",
      "======================================================================================\n",
      "\n",
      "Accuracy Criteria \n",
      "\n",
      "Probability Closer to 0 == Negative Sentiment\n",
      "Probability Closer to 1 == Positive Sentiment\n",
      "\n",
      " ==> Probability is 0.83420324 (Positive)\n"
     ]
    }
   ],
   "source": [
    "prediction = int(model.predict(tw).round().item())\n",
    "outcome = (review_labels_train[1][prediction]).capitalize()\n",
    "\n",
    "print(\"Actual Review: \" + test_word)\n",
    "print(\"\\nSentiment Analysis Outcome ==> The review shows \" + (review_labels_train[1][prediction]).capitalize() + \" sentiment.\")\n",
    "print(\"\\n======================================================================================\")\n",
    "\n",
    "print(\"\\nAccuracy Criteria \\n\\nProbability Closer to 0 == Negative Sentiment\\nProbability Closer to 1 == Positive Sentiment\")\n",
    "\n",
    "prob = model.predict(tw)[0][0]\n",
    "\n",
    "print(\"\\n ==> Probability is \" + str(prob)+ \" (\" + outcome + \")\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
